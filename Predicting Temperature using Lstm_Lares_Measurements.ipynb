{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bcfho8jZh2sD"
      },
      "source": [
        "### Time Series Forecasting using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OabeeefPilgR"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8,6)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orig_df = pd.read_csv('/Users/faymajidelhassan/Downloads/Master project /Data/Weather/measurements/Combinedweather_measurements.csv') \n",
        "df = orig_df.copy() \n",
        "print(f'Size of the dataset: {df.shape} \\n')  \n",
        "print() \n",
        "display(df.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "df.set_index('timestamp', inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.isnull().sum()\n",
        "# df=df.fillna(df.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df=df.fillna(df.mean())\n",
        "df = df.apply(lambda col: col.fillna(col.mean()), axis=0)\n",
        "\n",
        "df.isnull().sum()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kBZ2zKYtq2kl"
      },
      "source": [
        "Observations:\n",
        "1) One reading evrry 10 mins (from datatime column time diff for every record )\n",
        "2) 1day = 6*24 = 144 readings\n",
        "Task : Forecasting Temperature(in degree ) in future \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "colab_type": "code",
        "id": "zeJTeqgwim9v",
        "outputId": "4b8e9c0e-3138-4fc3-fca1-8d0b9ffd49bc"
      },
      "outputs": [],
      "source": [
        "## using univariate feature(Only temperature for given time)\n",
        "\n",
        "uni_data = df['temperature']\n",
        "# uni_data.index = df['timestamp']\n",
        "uni_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "colab_type": "code",
        "id": "zY4gmf8winE1",
        "outputId": "b8ac77c0-98d0-4428-f9d1-bc97b4e46230"
      },
      "outputs": [],
      "source": [
        "uni_data.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VLMYNIn-inKm"
      },
      "outputs": [],
      "source": [
        "uni_data = uni_data.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "uni_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "EPYIEY8MinTI",
        "outputId": "e6a0c6f3-f6a2-44a7-e1e0-f2a0d2f3f35b"
      },
      "outputs": [],
      "source": [
        "## train test split for simple time series moving window average\n",
        "train_split = int(len(uni_data) * 0.8)#205082 #43228\n",
        "tf.random.set_seed(13)\n",
        "\n",
        "### standardize data\n",
        "uni_data_mean = uni_data[:train_split].mean()\n",
        "uni_data_std = uni_data[:train_split].std()\n",
        "uni_data  = (uni_data - uni_data_mean)/ uni_data_std\n",
        "\n",
        "print(type(uni_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZPPGWMAFtjZp"
      },
      "source": [
        "Moving Window Average\n",
        "\n",
        "\n",
        "1.   Given last 20 values of observations(temp) , predict next observation\n",
        "2.   MWA: predict== AVG(last 20 values)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4MFlF_abinm0"
      },
      "outputs": [],
      "source": [
        "## utility functions\n",
        "\n",
        "## funtion to create data for univariate forecasting\n",
        "\n",
        "def univariate_data(dataset, start_idx , end_idx , history_size, target_size):\n",
        "  data = []\n",
        "  labels = []\n",
        "  start_idx  = start_idx + history_size\n",
        "  if end_idx is None:\n",
        "    end_idx = len(dataset)- target_size\n",
        "  for i in range(start_idx , end_idx):\n",
        "    idxs = range(i-history_size , i)\n",
        "    data.append(np.reshape(dataset[idxs] , (history_size, 1))) ### reshape data\n",
        "    labels.append(dataset[i+target_size])\n",
        "  return np.array(data), np.array(labels)\n",
        "\n",
        "uni_data_history = 20   ## last 50 values\n",
        "uni_data_future = 0     ## future data\n",
        "\n",
        "x_train_uni , y_train_uni = univariate_data(uni_data , 0 , train_split , uni_data_history , uni_data_future)\n",
        "\n",
        "x_val_uni , y_val_uni = univariate_data(uni_data , train_split , None ,uni_data_history , uni_data_future)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_uni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "61E_3coTin1t",
        "outputId": "3353ffc3-caf2-49d8-f89d-d61773cc1da8"
      },
      "outputs": [],
      "source": [
        "print(x_train_uni.shape , y_train_uni.shape)\n",
        "print(x_val_uni.shape , y_val_uni.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "colab_type": "code",
        "id": "jtbMSRusin70",
        "outputId": "8edffc0a-4a79-4949-d05d-f9de9f0e8e80"
      },
      "outputs": [],
      "source": [
        "print('Single window of history data' , x_train_uni[0])\n",
        "\n",
        "print('Target Temperature to predict ' , y_train_uni[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "colab_type": "code",
        "id": "A411EMdxioEV",
        "outputId": "c9675378-9c3f-498a-c440-d9dc3e54eaef"
      },
      "outputs": [],
      "source": [
        "### fucntion to create time steps\n",
        "def create_time_steps(length):\n",
        "  return list(range(-length,0))\n",
        "\n",
        "### function to plot time series data\n",
        "\n",
        "def plot_time_series(plot_data, delta , title):\n",
        "  labels = [\"History\" , 'True Future' , 'Model Predcited']\n",
        "  marker = ['.-' , 'rx' , 'go']\n",
        "  time_steps = create_time_steps(plot_data[0].shape[0])\n",
        "\n",
        "  if delta:\n",
        "    future = delta\n",
        "  else:\n",
        "    future = 0\n",
        "  plt.title(title)\n",
        "  for i , x in enumerate(plot_data):\n",
        "    if i :\n",
        "      plt.plot(future , plot_data[i] , marker[i], markersize = 10 , label = labels[i])\n",
        "    else:\n",
        "      plt.plot(time_steps, plot_data[i].flatten(), marker[i], label = labels[i])\n",
        "  plt.legend()\n",
        "  plt.xlim([time_steps[0], (future+5) *2])\n",
        "\n",
        "  plt.xlabel('Time_Step')\n",
        "  return plt\n",
        "\n",
        "plot_time_series([x_train_uni[0] , y_train_uni[0]] , 0 , 'Sample Example')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "colab_type": "code",
        "id": "b5X3X05xioK0",
        "outputId": "d4442f52-0016-4061-d113-c9223ddff459"
      },
      "outputs": [],
      "source": [
        "i = 20\n",
        "plot_time_series([x_train_uni[i], y_train_uni[i]] , 0 , 'Sample Example')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NiL0a-R5ioSI"
      },
      "outputs": [],
      "source": [
        "### Moving window average\n",
        "\n",
        "def MWA(history):\n",
        "  return np.mean(history)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "colab_type": "code",
        "id": "5O6tADOmioZ8",
        "outputId": "b064d6ce-87de-47ee-d48c-56565accba1d"
      },
      "outputs": [],
      "source": [
        "i = 20\n",
        "plot_time_series([x_train_uni[i] , y_train_uni[i] , MWA(x_train_uni[i])] , 0 , 'MWA predicted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oc-bXND--IuJ"
      },
      "source": [
        "Univariate time-series forecasting\n",
        "\n",
        "\n",
        "*   Only single feature as temperature(historical data)\n",
        "*   Task:  Given last 20 observations(history) , predict next temperature value \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "zPd4HuN-ior8",
        "outputId": "739b28f3-18c7-4db0-efce-447de9a5b884"
      },
      "outputs": [],
      "source": [
        "## prepare tensorflow dataset\n",
        "batch_size = 256\n",
        "buffer_size = 10000\n",
        "\n",
        "train_uni = tf.data.Dataset.from_tensor_slices((x_train_uni , y_train_uni))\n",
        "train_uni = train_uni.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
        "\n",
        "val_uni = tf.data.Dataset.from_tensor_slices((x_val_uni , y_val_uni))\n",
        "val_uni = val_uni.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
        "\n",
        "print(train_uni)\n",
        "print(val_uni)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "colab_type": "code",
        "id": "uB-3VgOJioyT",
        "outputId": "51383501-4165-4bd6-800f-b47269e8d88e"
      },
      "outputs": [],
      "source": [
        "## Define LSTM model \n",
        "\n",
        "lstm_model = tf.keras.models.Sequential([tf.keras.layers.LSTM(16 , input_shape = x_train_uni.shape[-2:]), \n",
        "                                         tf.keras.layers.Dense(1)])\n",
        "\n",
        "lstm_model.compile(optimizer = 'adam', loss = 'mae')\n",
        "\n",
        "steps = 200\n",
        "\n",
        "EPOCHS =10\n",
        "\n",
        "lstm_model.fit(train_uni , epochs = EPOCHS, steps_per_epoch = steps ,\n",
        "               validation_data = val_uni, validation_steps = 50)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "DKEA53lliov0",
        "outputId": "fb4cf444-7200-4992-9758-fa70c0043598"
      },
      "outputs": [],
      "source": [
        "for i , j in val_uni.take(5):\n",
        "  plot = plot_time_series([i[0].numpy() , j[0].numpy() , lstm_model.predict(i)[0]] ,0 , 'LSTM UNIVARIATE')\n",
        "  plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MTfs6GPbDOiL"
      },
      "source": [
        "Multivariate  and Single step Forecasting\n",
        "\n",
        "\n",
        "*   Task: Given 3 features(temp , pressure , and density) at each time step can we predict the temp in future at single time step\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "colab_type": "code",
        "id": "Hh2HnNjxiopi",
        "outputId": "a56dd85c-86c9-4e8a-aa7c-892eb4b342d1"
      },
      "outputs": [],
      "source": [
        "## features \n",
        "\n",
        "features_6 = ['temperature', 'humidity', 'pressure', 'global_irradiance', 'direct_irradiance', 'diffuse_irradiance','precipitation']\n",
        "\n",
        "features = df[features_6]\n",
        "# features.index = df['timestamp']\n",
        "features.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "colab_type": "code",
        "id": "Uaz4LXuUioiq",
        "outputId": "2ed6f021-f8e5-43a0-b301-21be42024f6d"
      },
      "outputs": [],
      "source": [
        "features.plot(subplots=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n5Ft3nfyiogy"
      },
      "outputs": [],
      "source": [
        "### standardize data\n",
        "dataset = features.values\n",
        "# dataset = np.array(features)\n",
        "\n",
        "data_mean = dataset[:train_split].mean(axis =0)\n",
        "\n",
        "data_std = dataset[:train_split].std(axis = 0)\n",
        "\n",
        "dataset = (dataset - data_mean)/data_std\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "i4LP5gdAiocw"
      },
      "outputs": [],
      "source": [
        "### create mutlivariate data\n",
        "\n",
        "def mutlivariate_data(dataset , target , start_idx , end_idx , history_size , target_size,\n",
        "                      step ,  single_step = False):\n",
        "  data = []\n",
        "  labels = []\n",
        "  start_idx = start_idx + history_size\n",
        "  if end_idx is None:\n",
        "    end_idx = len(dataset)- target_size\n",
        "  for i in range(start_idx , end_idx ):\n",
        "    idxs = range(i-history_size, i, step) ### using step\n",
        "    data.append(dataset[idxs])\n",
        "    if single_step:\n",
        "      labels.append(target[i+target_size])\n",
        "    else:\n",
        "      labels.append(target[i:i+target_size])\n",
        "\n",
        "  return np.array(data) , np.array(labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "qmdeBNcuioYX",
        "outputId": "d6f3a6a9-ff5b-4b45-8eeb-35a96b06293a"
      },
      "outputs": [],
      "source": [
        "# ### generate multivariate data\n",
        "\n",
        "history = 720\n",
        "future_target = 72\n",
        "STEP = 6\n",
        "\n",
        "x_train_ss , y_train_ss = mutlivariate_data(dataset , dataset[:, 1], 0, train_split, history,\n",
        "                                            future_target, STEP , single_step = True)\n",
        "\n",
        "x_val_ss , y_val_ss = mutlivariate_data(dataset , dataset[:,1] , train_split , None , history ,\n",
        "                                        future_target, STEP, single_step = True)\n",
        "\n",
        "\n",
        "\n",
        "print(x_train_ss.shape, y_train_ss.shape)\n",
        "print(x_val_ss.shape, y_val_ss.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "Ufyzs-nFioVO",
        "outputId": "246c3cbd-082b-4345-efbc-7863582bb575"
      },
      "outputs": [],
      "source": [
        "## tensorflow dataset\n",
        "\n",
        "train_ss = tf.data.Dataset.from_tensor_slices((x_train_ss, y_train_ss))\n",
        "train_ss = train_ss.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
        "\n",
        "val_ss = tf.data.Dataset.from_tensor_slices((x_val_ss, y_val_ss))\n",
        "val_ss = val_ss.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
        "\n",
        "print(train_ss)\n",
        "print(val_ss)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "colab_type": "code",
        "id": "Rb2wsAl_ioQ5",
        "outputId": "f3e45add-1257-406c-d926-824837951a7e"
      },
      "outputs": [],
      "source": [
        "### Modelling using LSTM\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "# Cosine Learning Rate Schedule\n",
        "initial_learning_rate = 0.001\n",
        "decay_steps = 1000  # Number of steps after which learning rate is decayed\n",
        "alpha = 0.0  # Minimum learning rate value as a fraction of the initial learning rate\n",
        "\n",
        "cosine_decay = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=initial_learning_rate,\n",
        "    decay_steps=decay_steps,\n",
        "    alpha=alpha\n",
        ")\n",
        "from keras.callbacks import EarlyStopping\n",
        "callbacks = EarlyStopping(\n",
        "    patience = 5 , \n",
        "    restore_best_weights = True , \n",
        "    monitor = 'val_loss'\n",
        ")\n",
        "# lstm_model = tf.keras.models.Sequential([tf.keras.layers.LSTM(16 , input_shape = x_train_uni.shape[-2:]), \n",
        "#                                          tf.keras.layers.Dense(1)])\n",
        "single_step_model = tf.keras.models.Sequential()\n",
        "\n",
        "single_step_model.add(tf.keras.layers.LSTM(32,input_shape = x_train_ss.shape[-2:]))\n",
        "# single_step_model.add(layers.LayerNormalization())\n",
        "# single_step_model.add(tf.keras.layers.LSTM(64,return_sequences=False))\n",
        "# single_step_model.add(layers.LayerNormalization())/\n",
        "# single_step_model.add(tf.keras.layers.Dense(2, activation=\"relu\"))\n",
        "single_step_model.add(tf.keras.layers.Dense(1))\n",
        "single_step_model.compile(optimizer = tf.keras.optimizers.Adam(), loss = 'mae')\n",
        "single_step_model_history = single_step_model.fit(train_ss, epochs = EPOCHS ,\n",
        "                                                  steps_per_epoch =steps,verbose=1, validation_data = val_ss,\n",
        "                                                  validation_steps = 50,callbacks=[callbacks])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "directory = '/Users/faymajidelhassan/Downloads/Master project /CODE/EDA/Saved_models'\n",
        "filename = 'Lstm_single_step_model_measure+precip.h5'\n",
        "model_path = f'{directory}/{filename}'\n",
        "single_step_model.save(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "colab_type": "code",
        "id": "6-z4nTiFioND",
        "outputId": "c79a043e-97bc-4662-a842-069fdbbb432c"
      },
      "outputs": [],
      "source": [
        "## plot train test loss \n",
        "\n",
        "def plot_loss(history , title):\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "\n",
        "  epochs = range(len(loss))\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, loss , 'b' , label = 'Train Loss')\n",
        "  plt.plot(epochs, val_loss , 'r' , label = 'Validation Loss')\n",
        "  plt.title(title)\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "plot_loss(single_step_model_history , 'Single Step Training and validation loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "b2lNfZZLioJG",
        "outputId": "b9ab67ae-4941-4ae1-a589-baa8bffebd18"
      },
      "outputs": [],
      "source": [
        "for x, y in val_ss.take(5):\n",
        "  plot = plot_time_series([x[0][:, 1].numpy(), y[0].numpy(),\n",
        "                    single_step_model.predict(x)[0]], 12,\n",
        "                   'Single Step Prediction')\n",
        "  plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D9Kz451TRYtL"
      },
      "source": [
        "Multi-variate & multi-step forecasting\n",
        "-> Generate multiple future values of temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "c9g7pWStioAV",
        "outputId": "de2d7375-4614-4369-c8a7-0469bf395648"
      },
      "outputs": [],
      "source": [
        "future_target = 72 # 72 future values\n",
        "x_train_multi, y_train_multi = mutlivariate_data(dataset, dataset[:, 1], 0,\n",
        "                                                 train_split, history,\n",
        "                                                 future_target, STEP)\n",
        "x_val_multi, y_val_multi = mutlivariate_data(dataset, dataset[:, 1],\n",
        "                                             train_split, None, history,\n",
        "                                             future_target, STEP)\n",
        "\n",
        "print(x_train_multi.shape)\n",
        "print(y_train_multi.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Fkwcf3UNin6l"
      },
      "outputs": [],
      "source": [
        "# TF DATASET\n",
        "\n",
        "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
        "train_data_multi = train_data_multi.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
        "\n",
        "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
        "val_data_multi = val_data_multi.batch(batch_size).repeat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "colab_type": "code",
        "id": "GPyypB20in0P",
        "outputId": "8dff268f-7ca0-4881-f4ee-704cd053f81e"
      },
      "outputs": [],
      "source": [
        "#plotting function\n",
        "def multi_step_plot(history, true_future, prediction):\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  num_in = create_time_steps(len(history))\n",
        "  num_out = len(true_future)\n",
        "  plt.grid()\n",
        "  plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
        "  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'b-',\n",
        "           label='True Future')\n",
        "  if prediction.any():\n",
        "    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'r-',\n",
        "             label='Predicted Future')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()\n",
        "  \n",
        "\n",
        "\n",
        "for x, y in train_data_multi.take(1):\n",
        "  multi_step_plot(x[0], y[0], np.array([0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "colab_type": "code",
        "id": "28MmEpt8TD12",
        "outputId": "5cd8459f-2c71-4856-8326-1e22ec4f8fa4"
      },
      "outputs": [],
      "source": [
        "multi_step_model = tf.keras.models.Sequential()\n",
        "multi_step_model.add(tf.keras.layers.LSTM(32,\n",
        "                                          return_sequences=True,\n",
        "                                          input_shape=x_train_multi.shape[-2:]))\n",
        "# multi_step_model.add(tf.keras.layers.LSTM(4,return_sequences=False, activation='relu'))\n",
        "multi_step_model.add(tf.keras.layers.LSTM(16,activation='relu')) # for 72 output\n",
        "multi_step_model.add(tf.keras.layers.Dense(72)) # for 72 outputs\n",
        "\n",
        "multi_step_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=cosine_decay,clipvalue=1.0), loss='mae')\n",
        "multi_step_model.summary()\n",
        "multi_step_history = multi_step_model.fit(train_data_multi, epochs=EPOCHS,\n",
        "                                          steps_per_epoch=steps,\n",
        "                                          validation_data=val_data_multi,\n",
        "                                          validation_steps=50,callbacks=[callbacks])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create a new model that outputs the features from the second LSTM layer\n",
        "feature_extractor = tf.keras.Model(inputs=multi_step_model.input,\n",
        "                                   outputs=multi_step_model.get_layer(index=0).output)  # LSTM layer at index 1\n",
        "\n",
        "# Summary of the feature extractor model\n",
        "feature_extractor.summary()\n",
        "\n",
        "# Example data for which you want to extract features (replace with actual data)\n",
        "x_val_example, y_val_example = next(iter(val_data_multi))  # Get a batch of validation data\n",
        "\n",
        "# Extract LSTM features\n",
        "lstm_features = feature_extractor.predict(x_val_example)\n",
        "\n",
        "# Print the shape of the extracted features\n",
        "print(f'LSTM features shape: {lstm_features.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "colab_type": "code",
        "id": "zaCJerqLinvP",
        "outputId": "266d7736-69ed-4dc6-f46a-f025417209f0"
      },
      "outputs": [],
      "source": [
        "plot_loss(multi_step_history, 'Multi-Step Training and validation loss')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "9ohuOxLrinkt",
        "outputId": "df84339c-c680-49e0-d80e-def97e196e87"
      },
      "outputs": [],
      "source": [
        "for x, y in val_data_multi.take(5):\n",
        "  multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "directory = '/Users/faymajidelhassan/Downloads/Master project /CODE/EDA/Saved_models'\n",
        "filename = 'Lstm_multi_step_model_measure+precip.h5'\n",
        "model_path = f'{directory}/{filename}'\n",
        "multi_step_model.save(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Define and train XGBoost model\n",
        "\n",
        "# Train XGBoost model\n",
        "model_xgb = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
        "model_xgb.fit(x_train_multi.reshape(x_train_multi.shape[0], -1), y_train_multi[:, -1])  # Using the last value of the future target for training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate LSTM model\n",
        "mae_lstm_single = single_step_model.evaluate(val_ss, steps=100)\n",
        "mae_lstm_multi=multi_step_model.evaluate(val_data_multi, steps=100)\n",
        "\n",
        "\n",
        "# Evaluate XGBoost model\n",
        "xgb_forecast = model_xgb.predict(x_val_multi.reshape(x_val_multi.shape[0], -1))\n",
        "mae_xgb = np.mean(np.abs(xgb_forecast - y_val_multi[:, -1]))\n",
        "\n",
        "# Calculate MAE for SARIMAX model\n",
        "# mae_sarimax = np.mean(np.abs(sarimax_forecast - df['temperature'][train_split:]))\n",
        "\n",
        "print(f\"MAE for single LSTM: {mae_lstm_single}\")\n",
        "print(f\"MAE for LSTM: {mae_lstm_multi}\")\n",
        "print(f\"MAE for XGBoost: {mae_xgb}\")\n",
        "# print(f\"MAE for SARIMAX: {mae_sarimax}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_forecast(true_values, lstm_pred, xgb_pred, ensemble_pred):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(true_values, label='True Values')\n",
        "    plt.plot(lstm_pred, label='LSTM Predictions')\n",
        "    plt.plot(xgb_pred, label='XGBoost Predictions')\n",
        "    plt.plot(ensemble_pred, label='Ensemble Predictions', linestyle='--')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "true_values = np.random.rand(100)\n",
        "# lstm_pred = np.random.rand(100)\n",
        "# xgb_pred = np.random.rand(100)\n",
        "ensemble_pred = np.random.rand(100)\n",
        "\n",
        "# Plot the forecasts\n",
        "plot_forecast(true_values, lstm_pred, xgb_pred, ensemble_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameters\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# history = 720\n",
        "# future_target = 72\n",
        "# STEP = 6\n",
        "# BATCH_SIZE = 128\n",
        "# BUFFER_SIZE = 10000\n",
        "# EPOCHS = 10\n",
        "# steps_per_epoch = 200\n",
        "# train_split = int(len(dataset) * 0.7)\n",
        "\n",
        "# # Fit StandardScaler on training data\n",
        "# scaler = StandardScaler()\n",
        "# scaler.fit(dataset[:train_split])\n",
        "\n",
        "# # Transform the entire dataset\n",
        "dataset_scaled = dataset\n",
        "\n",
        "# Function to create multivariate data\n",
        "def multivariate_data(dataset, target, start_index, end_index, history_size, target_size, step, single_step=False):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    start_index = start_index + history_size\n",
        "    if end_index is None:\n",
        "        end_index = len(dataset) - target_size\n",
        "\n",
        "    for i in range(start_index, end_index):\n",
        "        indices = range(i - history_size, i, step)\n",
        "        data.append(dataset[indices])\n",
        "\n",
        "        if single_step:\n",
        "            labels.append(target[i + target_size])\n",
        "        else:\n",
        "            labels.append(target[i:i + target_size])\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "# Create the multivariate data with the scaled dataset\n",
        "x_train_ss, y_train_ss = multivariate_data(dataset_scaled, dataset_scaled[:, 1], 0, train_split, history, future_target, STEP, single_step=True)\n",
        "x_val_ss, y_val_ss = multivariate_data(dataset_scaled, dataset_scaled[:, 1], train_split, None, history, future_target, STEP, single_step=True)\n",
        "\n",
        "# Check shapes\n",
        "print(x_train_ss.shape, y_train_ss.shape)\n",
        "print(x_val_ss.shape, y_val_ss.shape)\n",
        "\n",
        "# Define the dataset for training and validation\n",
        "train_ss = tf.data.Dataset.from_tensor_slices((x_train_ss, y_train_ss))\n",
        "val_ss = tf.data.Dataset.from_tensor_slices((x_val_ss, y_val_ss))\n",
        "\n",
        "# Batch the datasets\n",
        "train_ss = train_ss.cache().shuffle(buffer_size).batch(batch_size).repeat()\n",
        "val_ss = val_ss.batch(batch_size).repeat()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the GRU model\n",
        "def create_gru_model(input_shape):\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input(shape=input_shape),\n",
        "        tf.keras.layers.GRU(32,return_sequences=True),\n",
        "        tf.keras.layers.GRU(16,activation='relu'),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mae')\n",
        "    return model\n",
        "\n",
        "# Assume x_train_ss.shape is (num_samples, time_steps, features)\n",
        "input_shape = input_shape=x_train_multi.shape[-2:]\n",
        "gru_model = create_gru_model(input_shape)\n",
        "\n",
        "# Train the GRU model\n",
        "gru_model_history = gru_model.fit(train_ss, epochs=EPOCHS, steps_per_epoch=steps, validation_data=val_ss, validation_steps=50)\n",
        "\n",
        "# Evaluate GRU model\n",
        "mae_gru_single = gru_model.evaluate(val_ss, steps=100)\n",
        "print(\"Mean Absolute Error for GRU model:\", mae_gru_single)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dense, Dropout, Flatten, Input\n",
        "import tensorflow as tf\n",
        "\n",
        "# Transformer Block for time series forecasting\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"), \n",
        "            Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1, training=training)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Build Transformer model\n",
        "def build_transformer_model(input_shape, embed_dim):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # Add a Dense layer to project input to the embedding dimension\n",
        "    x = Dense(embed_dim)(inputs)\n",
        "    x = TransformerBlock(embed_dim=embed_dim, num_heads=2, ff_dim=32)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(20, activation=\"relu\")(x)\n",
        "    outputs = Dense(1)(x)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Build and compile Transformer model\n",
        "input_shape = x_train_multi.shape[-2:]  # Correct input shape\n",
        "embed_dim = 32  # Embedding dimension\n",
        "transformer_model = build_transformer_model(input_shape, embed_dim)\n",
        "transformer_model.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "# Train Transformer model\n",
        "transformer_model_history = transformer_model.fit(train_ss, epochs=EPOCHS, steps_per_epoch=steps,\n",
        "                                                  validation_data=val_ss, validation_steps=50)\n",
        "\n",
        "# Evaluate Transformer model\n",
        "mae_transformer_single = transformer_model.evaluate(val_ss, steps=100)\n",
        "print(\"Mean Absolute Error for Transformer model:\", mae_transformer_single)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_time_series(data, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    time_steps = range(len(data[0]))  # History length\n",
        "    plt.plot(time_steps, data[0], label='History')  # Plot the historical data\n",
        "    plt.plot(len(data[0]), data[1], 'bo', label='True Future')  # Plot the true future value as a single point\n",
        "    plt.plot(len(data[0]), data[2], 'ro', label='Predicted Future')  # Plot the predicted future value as a single point\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    return plt\n",
        "\n",
        "# Plotting function for Transformer predictions\n",
        "for x, y in val_ss.take(5):\n",
        "    prediction = transformer_model.predict(x)\n",
        "    plot = plot_time_series([x[0].numpy(), y[0].numpy(), prediction[0]], 'Transformer UNIVARIATE')\n",
        "    plot.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "w8y2R6zVim0X"
      },
      "outputs": [],
      "source": [
        "# Evaluate GRU model\n",
        "mae_gru_single = gru_model.evaluate(val_ss, steps=100)\n",
        "print(f\"MAE for GRU: {mae_gru_single}\")\n",
        "\n",
        "# Evaluate Transformer model\n",
        "mae_transformer_single = transformer_model.evaluate(val_ss, steps=100)\n",
        "print(f\"MAE for Transformer: {mae_transformer_single}\")\n",
        "\n",
        "\n",
        "# Generate predictions for each model\n",
        "pred_multi_step = multi_step_model.predict(val_data_multi, steps=100)[:, 0]\n",
        "pred_xgb = model_xgb.predict(x_val_multi.reshape(x_val_multi.shape[0], -1))\n",
        "pred_gru = gru_model.predict(val_ss, steps=100)[:, 0]\n",
        "pred_transformer = transformer_model.predict(val_ss, steps=100)[:, 0]\n",
        "\n",
        "# Check for NaNs in predictions\n",
        "print(\"NaNs in pred_multi_step:\", np.isnan(pred_multi_step).sum())\n",
        "print(\"NaNs in pred_xgb:\", np.isnan(pred_xgb).sum())\n",
        "print(\"NaNs in pred_gru:\", np.isnan(pred_gru).sum())\n",
        "print(\"NaNs in pred_transformer:\", np.isnan(pred_transformer).sum())\n",
        "\n",
        "# Ensure all predictions have the same length\n",
        "# Ensure all predictions have the same length\n",
        "min_length = min(len(pred_multi_step), len(pred_xgb), len(pred_gru), len(pred_transformer))\n",
        "\n",
        "pred_multi_step = pred_multi_step[:min_length]\n",
        "pred_xgb = pred_xgb[:min_length]\n",
        "pred_gru = pred_gru[:min_length]\n",
        "pred_transformer = pred_transformer[:min_length]\n",
        "\n",
        "# Combine predictions from all models\n",
        "combined_forecast = (pred_multi_step + pred_xgb + pred_gru + pred_transformer) / 4\n",
        "\n",
        "# Check for NaNs in combined forecast\n",
        "print(\"NaNs in combined_forecast:\", np.isnan(combined_forecast).sum())\n",
        "# Ensure true values have the same length as combined_forecast\n",
        "true_values = dataset_scaled[temperature][train_split:train_split + min_length]\n",
        "\n",
        "# Check for NaNs in true values\n",
        "print(\"NaNs in true_values:\", np.isnan(true_values).sum())\n",
        "\n",
        "\n",
        "# Calculate MAE for ensemble model\n",
        "# true_values = df['temperature'][train_split:train_split + min_length].values\n",
        "# Remove NaNs from combined forecast and true values\n",
        "# Remove NaNs from combined forecast and true values\n",
        "if np.isnan(combined_forecast).sum() > 0 or np.isnan(true_values).sum() > 0:\n",
        "    valid_indices = ~np.isnan(combined_forecast) & ~np.isnan(true_values)\n",
        "    combined_forecast = combined_forecast[valid_indices]\n",
        "    true_values = true_values[valid_indices]\n",
        "\n",
        "# Calculate MAE for each feature separately\n",
        "mae_ensemble = np.mean(np.abs(combined_forecast - true_values), axis=0)\n",
        "\n",
        "# Print MAE for each feature\n",
        "for i, mae in enumerate(mae_ensemble):\n",
        "    print(f\"MAE for {features_6[i]}: {mae}\")\n",
        "\n",
        "# Optionally, you can calculate the overall MAE by averaging the MAE for all features\n",
        "overall_mae_ensemble = np.mean(mae_ensemble)\n",
        "print(f\"Overall MAE for Ensemble Model: {overall_mae_ensemble}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "# Perform Randomized Search\n",
        "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, n_iter=50, cv=3, verbose=1, n_jobs=-1)\n",
        "random_search.fit(x_train_multi.reshape(x_train_multi.shape[0], -1), y_train_multi[:, -1])\n",
        "\n",
        "# Best parameters\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best parameters: {best_params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to evaluate model on test data\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    predictions = model.predict(x_test)\n",
        "    mae = np.mean(np.abs(predictions - y_test))\n",
        "    rmse = np.sqrt(np.mean((predictions - y_test)**2))\n",
        "    return mae, rmse\n",
        "\n",
        "# Evaluate models\n",
        "mae_lstm, rmse_lstm = evaluate_model(lstm_model, x_val_uni, y_val_uni)\n",
        "mae_gru, rmse_gru = evaluate_model(gru_model, x_val_ss, y_val_ss)\n",
        "mae_transformer, rmse_transformer = evaluate_model(transformer_model, x_val_ss, y_val_ss)\n",
        "mae_xgb, rmse_xgb = evaluate_model(model_xgb, x_val_multi.reshape(x_val_multi.shape[0], -1), y_val_multi[:, -1])\n",
        "\n",
        "print(f\"LSTM MAE: {mae_lstm}, RMSE: {rmse_lstm}\")\n",
        "print(f\"GRU MAE: {mae_gru}, RMSE: {rmse_gru}\")\n",
        "print(f\"Transformer MAE: {mae_transformer}, RMSE: {rmse_transformer}\")\n",
        "print(f\"XGBoost MAE: {mae_xgb}, RMSE: {rmse_xgb}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_forecast(true_values, lstm_pred, gru_pred, transformer_pred, xgb_pred, ensemble_pred):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(true_values, label='True Values')\n",
        "    plt.plot(lstm_pred, label='LSTM Predictions')\n",
        "    plt.plot(gru_pred, label='GRU Predictions')\n",
        "    plt.plot(transformer_pred, label='Transformer Predictions')\n",
        "    plt.plot(xgb_pred, label='XGBoost Predictions')\n",
        "    plt.plot(ensemble_pred, label='Ensemble Predictions', linestyle='--')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Generate predictions for visualization\n",
        "true_values = y_val_multi[:, -1]\n",
        "lstm_pred = lstm_model.predict(x_val_multi[:, -1])\n",
        "gru_pred = gru_model.predict(x_val_ss, steps=100)[:, 0]\n",
        "transformer_pred = transformer_model.predict(x_val_ss, steps=100)[:, 0]\n",
        "xgb_pred = model_xgb.predict(x_val_multi.reshape(x_val_multi.shape[0], -1))\n",
        "# ensemble_pred = ensemble_model.predict(x_val_multi.reshape(x_val_multi.shape[0], -1))\n",
        "\n",
        "# Plot the forecasts\n",
        "plot_forecast(true_values, lstm_pred, gru_pred, transformer_pred, xgb_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cR1L9pvfimt4"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# # Assuming you want to use temperature data from x_train\n",
        "endog = x_train[:, :, 0].reshape(-1)  # Selecting the first column (temperature) as endogenous variable and reshaping to 1D\n",
        "\n",
        "# # Define SARIMAX model\n",
        "model = SARIMAX(endog, order=(0, 1, 3), seasonal_order=(0, 1, 1, 12))\n",
        "\n",
        "# Fit the model\n",
        "# results = model.fit()\n",
        "# results = model.fit(maxiter=1000)  # Increase maxiter to 1000 (or higher)\n",
        "results = model.fit(method='powell')  # Try using the 'powell' method\n",
        "\n",
        "# Print summary\n",
        "print(results.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "\n",
        "# Define a function to perform grid search for SARIMAX model\n",
        "# Define a function to perform grid search for SARIMAX model\n",
        "def sarimax_grid_search(p_values, d_values, q_values, P_values, D_values, Q_values, m_values, train, test):\n",
        "    # Create a DataFrame to store the results\n",
        "    column_names = ['p', 'd', 'q', 'P', 'D', 'Q', 'm', 'RMSE_train', 'RMSE_test']\n",
        "    df = pd.DataFrame(columns=column_names)\n",
        "    \n",
        "    # Iterate through all combinations of hyperparameters\n",
        "    for p in p_values:\n",
        "        for d in d_values:\n",
        "            for q in q_values:\n",
        "                for P in P_values:\n",
        "                    for D in D_values:\n",
        "                        for Q in Q_values:\n",
        "                            for m in m_values:\n",
        "                                # Fit SARIMAX model\n",
        "                                try:\n",
        "                                    model = SARIMAX(train, order=(p, d, q), seasonal_order=(P, D, Q, m))\n",
        "                                    results = model.fit(method='powell')  # Increase maxiter to 1000 (or higher\n",
        "                                except Exception as e:\n",
        "                                    print(f\"Error fitting SARIMAX model with parameters ({p}, {d}, {q}), ({P}, {D}, {Q}, {m}): {e}\")\n",
        "                                    continue\n",
        "                                \n",
        "                              \n",
        "                                # Make predictions on train and test datasets\n",
        "                                train_predictions = pd.Series(results.predict(start=1, end=train_data_len-1), name='Predictions')\n",
        "                                test_predictions = results.predict(start=len(x_train), end=len(x_train)+len(x_test)-1)\n",
        "\n",
        "                                \n",
        "                                # Calculate RMSE values\n",
        "                                RMSE_train = np.sqrt(mean_squared_error(train_predictions, train[:train_data_len-1]))  # Only consider the first 48 data points for training\n",
        "                                RMSE_test = np.sqrt(mean_squared_error(test_predictions, test))\n",
        "\n",
        "                                \n",
        "                                # Append results to the DataFrame\n",
        "                                # Append results to the DataFrame\n",
        "                                new_row = pd.DataFrame({'p': [p], 'd': [d], 'q': [q],\n",
        "                                                        'P': [P], 'D': [D], 'Q': [Q], 'm': [m],\n",
        "                                                        'RMSE_train': [RMSE_train], 'RMSE_test': [RMSE_test]})\n",
        "                                df = pd.concat([df, new_row], ignore_index=True)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train = x_train[:, :, 0].reshape(-1)  # Use the temperature data from x_train as the endogenous variable\n",
        "test = y_test_temp  # Use the temperature data from y_test as the testing dataset\n",
        "# Define hyperparameters for grid search\n",
        "# p_values = [0, 1, 2]\n",
        "# d_values = [0, 1]\n",
        "# q_values = [0, 1, 2]\n",
        "# P_values = [0, 1, 2]\n",
        "# D_values = [0, 1]\n",
        "# Q_values = [0, 1, 2]\n",
        "# m_values = [12]\n",
        "\n",
        "# Define seasonal hyperparameters AND TRY AFTER THE ABOVE \n",
        "p = [1,2,3,0]\n",
        "d = [1,2,3,0]\n",
        "q = [1,2,3,0]\n",
        "P = [1,2,3,0]\n",
        "D = [1,2,3,0] # Seasonal\n",
        "Q = [1,2,3,0] # Seasonal\n",
        "m = [12]  # Monthly seasonal cycle\n",
        "\n",
        "# Disable warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Perform grid search\n",
        "grid_results = sarimax_grid_search(p, d, q, P, D, Q, m, endog, test)\n",
        "\n",
        "# Display grid search results\n",
        "print(grid_results)\n",
        "\n",
        "# Save grid search results to a CSV file\n",
        "grid_results.to_csv(\"sarimax_grid_search_results.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Time Series Forecasting (Predicting Temperature) using LSTM .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
